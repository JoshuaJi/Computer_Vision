{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Stitching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a class to stitch two images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class Stitcher:\n",
    "    # the __init__ function is automatically called upon instantiation\n",
    "    # is't a good practice to initialize/reset all class members here\n",
    "    def __init__(self):               \n",
    "        # create a SIFT object\n",
    "        self.sift = cv2.xfeatures2d.SIFT_create()\n",
    "        # create a Brute-Force Matcher\n",
    "        self.bf = cv2.BFMatcher(cv2.NORM_L2)\n",
    "      \n",
    "    def featureExtraction(self):        \n",
    "        self.kp1,self.desc1 = self.sift.detectAndCompute(self.img1,None)\n",
    "        self.kp2,self.desc2 = self.sift.detectAndCompute(self.img2,None)\n",
    "        \n",
    "    def matchDescriptors(self):\n",
    "        self.matches = self.bf.match(self.desc1,self.desc2)\n",
    "        \n",
    "    def displayMatches(self, N):\n",
    "        # Sort them in the order of their distance\n",
    "        matches = sorted(self.matches, key = lambda x:x.distance)        \n",
    "        # Draw the best N matches\n",
    "        img = cv2.drawMatches(self.img1,self.kp1,self.img2,self.kp2,matches[:N],None,flags=2)\n",
    "        cv2.imshow(\"Matched keypoints\",img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "    def homography(self):\n",
    "        # We need to convert the matched points (cv.DMatch) into Nx2 Numpy arrays\n",
    "        # A cv2.DMatch object contains distance, queryIdx and trainIdx\n",
    "        # distance: The Euclidean distance between the the descriptors from the two sets\n",
    "        # queryIdx: The index of the matched descriptor in the query set (1st set)\n",
    "        # trainIdx:: The index of the matched descriptor in the train set (2nd set)\n",
    "        \n",
    "        # local placeholders for the matched points coordinates\n",
    "        srcPts =  np.empty((len(self.matches),2))\n",
    "        dstPts =  np.empty((len(self.matches),2))\n",
    "        for i in range(len(self.matches)):\n",
    "            srcPts[i,:] = np.float32(self.kp1[self.matches[i].queryIdx].pt)\n",
    "            dstPts[i,:] = np.float32(self.kp2[self.matches[i].trainIdx].pt)\n",
    "        \n",
    "        # compute the 3x3 transformation matrix to map dstPts into srcPts (note the ordering)\n",
    "        self.M, mask = cv2.findHomography(dstPts, srcPts, cv2.RANSAC,5.0)\n",
    "        \n",
    "    def findOutputLimits(self):\n",
    "        # we need to find the projected image corners to determine the size of the panorama image\n",
    "        # the projected corners might map to negative pixel coordinates -> translate the image\n",
    "        \n",
    "        # the four corners are [0,0], [0,height-1], [width-1,0] and [width-1,height-1]\n",
    "        # [x_proj y_proj 1] = M * [x_corner y_corner 1]\n",
    "        \n",
    "        # top left\n",
    "        tl = np.dot(self.M, np.array([0,0,1]))\n",
    "        tl = tl/tl[-1]           \n",
    "        # top right\n",
    "        tr = np.dot(self.M, np.array([self.img2.shape[1]-1,0,1]))\n",
    "        tr = tr/tr[-1]        \n",
    "        # bottom left\n",
    "        bl = np.dot(self.M, np.array([0, self.img2.shape[0]-1,1]))\n",
    "        bl = bl/bl[-1]        \n",
    "        # bottom right\n",
    "        br = np.dot(self.M, np.array([self.img2.shape[1]-1, self.img2.shape[0]-1,1]))\n",
    "        br = br/br[-1]\n",
    "        \n",
    "        # find the xMin and yMin\n",
    "        self.xMin = min(tl[0],tr[0],bl[0],br[0],0)\n",
    "        self.xMax = max(tl[0],tr[0],bl[0],br[0],self.img1.shape[0])\n",
    "        self.yMin = min(tl[1],tr[1],bl[1],br[1],0)\n",
    "        self.yMax = max(tl[1],tr[1],bl[1],br[1],self.img1.shape[1])\n",
    "\n",
    "        # create a 3x3 translation matrix\n",
    "        # [ [1 0 -xMin], [0 1 -yMin], [0 0 1] ]\n",
    "        self.T = np.array([[1,0,-self.xMin],[0,1,-self.yMin],[0,0,1]])        \n",
    "        \n",
    "        # apply the translation matrix to the transformation matrix\n",
    "        #  M <- T * M      \n",
    "        self.M = np.dot(self.T,self.M)        \n",
    "\n",
    "        # compute the panorama size        \n",
    "        self.panoSize = (int(self.xMax-self.xMin+1),int(self.yMax-self.yMin+1))                  \n",
    "        \n",
    "    def perspectiveWarp(self):\n",
    "        # warp the second image into the panorama\n",
    "        self.pano = cv2.warpPerspective(self.img2, self.M, self.panoSize)\n",
    "        # no blending, just copy the left-side image into the panorama\n",
    "        self.pano[self.yMin:self.img1.shape[0]+self.yMin,self.xMin:self.img1.shape[1]+self.xMin] = self.img1             \n",
    "        \n",
    "    def doStitch(self,image1,image2):\n",
    "        # make a deep copy of the input images\n",
    "        self.img1 = image1.copy()\n",
    "        self.img2 = image2.copy() \n",
    "        # compute keypoints/descriptors\n",
    "        self.featureExtraction()        \n",
    "        # match descriptors\n",
    "        self.matchDescriptors()\n",
    "        # compute the transformation matrix\n",
    "        self.homography()\n",
    "        # translate the images and compute the panorama size\n",
    "        self.findOutputLimits() \n",
    "        # apply a perspective warp to stitch the images\n",
    "        self.perspectiveWarp()                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create an object of the Stitcher class, pass the images to it to compute the panorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-93bbdd701c55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStitcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# feed the images to the main method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoStitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# display the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2ab6f3eb4b5b>\u001b[0m in \u001b[0;36mdoStitch\u001b[0;34m(self, image1, image2)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindOutputLimits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# apply a perspective warp to stitch the images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperspectiveWarp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-2ab6f3eb4b5b>\u001b[0m in \u001b[0;36mperspectiveWarp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpano\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarpPerspective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpanoSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# no blending, just copy the left-side image into the panorama\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpano\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myMin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myMin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxMin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxMin\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdoStitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the image \n",
    "img1 = cv2.imread('GB1.jpg')\n",
    "img2 = cv2.imread('GB2.jpg')\n",
    "\n",
    "# Stitcher object\n",
    "s = Stitcher()\n",
    "# feed the images to the main method \n",
    "s.doStitch(img1,img2)\n",
    "\n",
    "# display the results\n",
    "cv2.imshow(\"Pano\", s.pano)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
